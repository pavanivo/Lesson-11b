{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "#  K-Nearest Neighbors with `scikit-learn`\n",
    "\n",
    "_Authors: Alex Sherman (DC)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "\n",
    "1. Utilize the KNN model on the iris data set.\n",
    "2. Implement scikit-learn's KNN model.\n",
    "3. Assess the fit of a KNN Model using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Learning Objectives](#learning-objectives)\n",
    "- [Loading the Iris Data Set](#overview-of-the-iris-dataset)\n",
    "\t- [Terminology](#terminology)\n",
    "- [Exercise: \"Human Learning\" With Iris Data](#exercise-human-learning-with-iris-data)\n",
    "- [Human Learning on the Iris Data Set](#human-learning-on-the-iris-dataset)\n",
    "- [K-Nearest Neighbors (KNN) Classification](#k-nearest-neighbors-knn-classification)\n",
    "\t- [Using the Train/Test Split Procedure (K=1)](#using-the-traintest-split-procedure-k)\n",
    "- [Tuning a KNN Model](#tuning-a-knn-model)\n",
    "\t- [What Happens If We View the Accuracy of our Training Data?](#what-happen-if-we-view-the-accuracy-of-our-training-data)\n",
    "\t- [Training Error Versus Testing Error](#training-error-versus-testing-error)\n",
    "- [Standardizing Features](#standardizing-features)\n",
    "\t- [Use `StandardScaler` to Standardize our Data](#use-standardscaler-to-standardize-our-data)\n",
    "- [Comparing KNN With Other Models](#comparing-knn-with-other-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will get an intuitive and practical feel for the **k-Nearest Neighbors** model. kNN is a **non-parametric model**. So, the model is not represented as an equation with parameters (e.g. the $\\beta$ values in linear regression).\n",
    "\n",
    "First, we will make a model by hand to classify iris flower data. Next, we will automatedly make a model using kNN.\n",
    "\n",
    "> You may have heard of the clustering algorithm **k-Means Clustering**. These techniques have nothing in common, aside from both having a parameter k!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-nearest-neighbors-knn-classification\"></a>\n",
    "## K-Nearest Neighbors (KNN) Classification\n",
    "---\n",
    "\n",
    "K-nearest neighbors classification is (as its name implies) a classification model that uses the \"K\" most similar observations in order to make a prediction.\n",
    "\n",
    "KNN is a supervised learning method; therefore, the training data must have known target values.\n",
    "\n",
    "The process of prediction using KNN is fairly straightforward:\n",
    "\n",
    "1. Pick a value for K.\n",
    "2. Search for the K observations in the data that are \"nearest\" to the measurements of the unknown iris.\n",
    "    - Euclidian distance is often used as the distance metric, but other metrics are allowed.\n",
    "3. Use the most popular response value from the K \"nearest neighbors\" as the predicted response value for the unknown iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations below show how a given area can change in its prediction as K changes.\n",
    "\n",
    "- Colored points represent true values and colored areas represent a **prediction space**. (This is called a Voronoi Diagram.)\n",
    "- Each prediction space is wgere the majority of the \"K\" nearest points are the color of the space.\n",
    "- To predict the class of a new point, we guess the class corresponding to the color of the space it lies in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn-classification-map-for-iris-k\"></a>\n",
    "### KNN Classification Map for Iris (K=1)\n",
    "\n",
    "![1NN classification map](../assets/knn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification Map for Iris (K=3)\n",
    "\n",
    "![5NN classification map](../assets/knn_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification Map for Iris (K=5)\n",
    "\n",
    "![15NN classification map](../assets/knn_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn-classification-map-for-iris-k\"></a>\n",
    "### KNN Classification Map for Iris (K=7)\n",
    "\n",
    "![50NN classification map](../assets/knn_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as K increases, the classification spaces' borders become more distinct. However, you can also see that the spaces are not perfectly pure when it comes to the known elements within them.\n",
    "\n",
    "**How are outliers affected by K?** As K increases, outliers are \"smoothed out\". Look at the above three plots and notice how outliers strongly affect the prediction space when K=1. When K=50, outliers no longer affect region boundaries. This is a classic bias-variance tradeoff -- with increasing K, the bias increases but the variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's the \"best\" value for K in this case?\n",
    "\n",
    "**Answer:** \n",
    "The value which produces the most accurate predictions on **unseen data**. We want to create a model that generalizes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the lesson, we will be using a dataset containing the 2015 season statistics for ~500 NBA players. This dataset leads to a nice choice of K, as we'll see below. The columns we'll use for features (and the target 'pos') are:\n",
    "\n",
    "\n",
    "| Column | Meaning |\n",
    "| ---    | ---     |\n",
    "| pos | C: Center. F: Front. G: Guard |\n",
    "| ast | Assists per game | \n",
    "| stl | Steals per game | \n",
    "| blk | Blocks per game |\n",
    "| tov | Turnovers per game | \n",
    "| pf  | Personal fouls per game | \n",
    "\n",
    "For information about the other columns, see [this glossary](https://www.basketball-reference.com/about/glossary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NBA data into a DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "path = \n",
    "nba = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map positions to numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response vector (y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"using-the-traintest-split-procedure-k\"></a>\n",
    "### Using the Train/Test Split Procedure (K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set (using K=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** If we had trained on the entire dataset and tested on the entire dataset, using 1-KNN what accuracy would we likely get? If the resulting accuracy is not this number, what must some data points look like?\n",
    "\n",
    "**Answer:** We would expect nearly 100% accuracy. The points tested on would have a distance of zero from themselves (in the training set), so the correct classes would be predicted! If we get less than 100% accuracy, then we must have some points in our data set that are the same point but a different class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Repeating for K=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Suppose we again train and test on the entire data set, but using 50-KNN. Would we expect the accuracy to be higher, lower, or the same as compared to 1-KNN?\n",
    "\n",
    "**Answer:** The accuracy will decrease. 50-KNN would start misclassifying points. We used to know the correct answers since each points is a distance of zero from itself! Now, we take the majority class of the closest 50 points, so we are bound to get many more incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparing Testing Accuracy With Null Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Null accuracy is the accuracy that can be achieved by **always predicting the most frequent class**. For example, if most players in our data set are Centers, we would always predict Center.\n",
    "\n",
    "The null accuracy is a benchmark against which you may want to measure every classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the class distribution from the training set.\n",
    "\n",
    "Remember that we are comparing KNN to this simpler model. So, we must find the most frequent class **of the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-happen-if-we-view-the-accuracy-of-our-training-data\"></a>\n",
    "### What Happens If We View the Accuracy of our Training Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to store results\n",
    "\n",
    "\n",
    "# loop through 100 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As K increases, why does the accuracy fall?\n",
    "\n",
    "**Answer:** More points are used to find each class. At some point, increasing K stops eliminating outliers and starts smoothing over the actual class boundaries, resulting in mispredictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Search for the \"best\" value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAINING ERROR and TESTING ERROR for K=1 through 100.\n",
    "\n",
    "\n",
    "\n",
    "# Find test accuracy for all values of K between 1 and 100 (inclusive).\n",
    "for k in k_range:\n",
    "\n",
    "    # Instantiate the model with the current K value.\n",
    "\n",
    "    \n",
    "    # Calculate training error (error = 1 - accuracy).\n",
    "\n",
    "    \n",
    "    # Calculate testing error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of K, training error, and testing error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and TESTING ERROR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum testing error and the associated K value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-error-versus-testing-error\"></a>\n",
    "### Training Error Versus Testing Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and both TRAINING ERROR and TESTING ERROR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error** decreases as model complexity increases (lower value of K).\n",
    "- **Testing error** is minimized at the optimum model complexity.\n",
    "\n",
    "Evaluating the training and testing error is important. For example:\n",
    "\n",
    "- If the training error is much lower than the test error, then our model is likely overfitting. \n",
    "- If the test error starts increasing as we vary a hyperparameter, we may be overfitting.\n",
    "- If either error plateaus, our model is likely underfitting (not complex enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Making Predictions on Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the statistics of a (truly) unknown NBA player, how do we predict his position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instantiate the model with the best-known parameters.\n",
    "\n",
    "# Re-train the model with X and y (not X_train and y_train). Why?\n",
    "\n",
    "# Make a prediction for an out-of-sample observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What could we conclude?\n",
    "\n",
    "- When using KNN on this data set with these features, the **best value for K** is likely to be around 14.\n",
    "- Given the statistics of an **unknown player**, we estimate that we would be able to correctly predict his position about 74% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"standardizing-features\"></a>\n",
    "## Standardizing Features\n",
    "---\n",
    "\n",
    "There is one major issue that applies to many machine learning models: They are sensitive to feature scale. \n",
    "\n",
    "> KNN in particular is sensitive to feature scale because it (by default) uses the Euclidean distance metric. To determine closeness, Euclidean distance sums the square difference along each axis. So, if one axis has large differences and another has small differences, the former axis will contribute much more to the distance than the latter axis.\n",
    "\n",
    "This means that it matters whether our feature are centered around zero and have similar variance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, most data does not naturally start at a mean of zero and a shared variance. Other models tend to struggle with scale as well, even linear regression, when you get into more advanced methods such as regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortuantely, this is an easy fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use-standardscaler-to-standardize-our-data\"></a>\n",
    "### Use `StandardScaler` to Standardize our Data\n",
    "\n",
    "StandardScaler standardizes our data by subtracting the mean from each feature and dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate feature matrix and response for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n",
    "feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "\n",
    "X = nba[feature_cols]\n",
    "y = nba.pos_num  # Create response vector (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the train/test split.\n",
    "\n",
    "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform!!!!\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a KNN model and look at the testing error.\n",
    "Can you find a number of neighbors that improves our results from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small gain from 25.8 on test data previously, but it goes to prove that feature engineering can do more for an algorithm than a more advanced algo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-knn-with-other-models\"></a>\n",
    "## Comparing KNN With Other Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of KNN:**\n",
    "\n",
    "- It's simple to understand and explain.\n",
    "- Model training is fast.\n",
    "- It can be used for classification and regression (for regression, take the average value of the K nearest points!).\n",
    "- Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "**Disadvantages of KNN:**\n",
    "\n",
    "- It must store all of the training data.\n",
    "- Its prediction phase can be slow when n is large.\n",
    "- It is sensitive to irrelevant features.\n",
    "- It is sensitive to the scale of the data.\n",
    "- Accuracy is (generally) not competitive with the best supervised learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
